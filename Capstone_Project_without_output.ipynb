{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36598cc0",
   "metadata": {},
   "source": [
    "# Prediction of deaths due to Heart Failures\n",
    "\n",
    "In today's world, Cardiovascular diseases (CVDs) are one of the most common causes of death.It is a class of diseases that involve the heart or blood vessels.Together CVD resulted in 17.9 million deaths (32.1%) in 2015, up from 12.3 million (25.8%) in 1990. Deaths, at a given age, from CVD are more common and have been increasing in much of the developing world, while rates have declined in most of the developed world since the 1970s. Coronary artery disease and stroke account for 80% of CVD deaths in males and 75% of CVD deaths in females. Most cardiovascular disease affects older adults. In the United States 11% of people between 20 and 40 have CVD, while 37% between 40 and 60, 71% of people between 60 and 80, and 85% of people over 80 have CVD. The average age of death from coronary artery disease in the developed world is around 80 while it is around 68 in the developing world. Diagnosis of disease typically occurs seven to ten years earlier in men as compared to women. Amongst all types of CVDs, heart failure is one of the most common effect observed. Most cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\n",
    "\n",
    "\n",
    "- The data that I have used contains 12 features that can be used to predict mortality by heart failure.\n",
    "- This project is a binary classification problem.\n",
    "- We will predict the Heart Failures using different models (our target variable being Death_Event).\n",
    "- Target variable is the variable that is or should be the output. In our study our target variable is Death Event in the contex of determining whether anybody is likely to die due to heart failures based on the input parameters like gender, age and various test results or not.\n",
    "- Lastly, we will build a variety of Classification models and compare the models giving the best prediction on Heart Failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bbaec3",
   "metadata": {},
   "source": [
    "# Contents:\n",
    "- Libraries\n",
    "- Data Load\n",
    "- Data Pre-processing and Cleaning\n",
    "- EDA and Feature Engineering\n",
    "- Model Section and Training\n",
    "- Hyper parameter Tuning\n",
    "- Final Model\n",
    "- Results and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc94cd0",
   "metadata": {},
   "source": [
    "# 1. Libraries\n",
    "\n",
    "All the libraries required for the script are imported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77b6583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import seaborn as sb\n",
    "import plotly.express as px\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c7b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import GridSearchCV, HalvingGridSearchCV, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RepeatedKFold, RepeatedStratifiedKFold, cross_val_score, cross_validate\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, mutual_info_score\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_curve,confusion_matrix, ConfusionMatrixDisplay,auc, RocCurveDisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6692dc5b",
   "metadata": {},
   "source": [
    "# 2. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be9d7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"heart_failure_clinical_records_dataset.csv\")\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ddab8",
   "metadata": {},
   "source": [
    "# 3. Data Pre-processing and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef9f6ad",
   "metadata": {},
   "source": [
    "#### Dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15420090",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2396e8",
   "metadata": {},
   "source": [
    "#### Dataset Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b54d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1038412",
   "metadata": {},
   "source": [
    "#### We extract the first 5 rows of the dataset using the head() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb773ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519b97a",
   "metadata": {},
   "source": [
    "#### We extract the last 5 rows of the dataset using the tail() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9347a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59c567b",
   "metadata": {},
   "source": [
    "#### Next, we extract the information from our dataset using the describe() function.\n",
    "\n",
    "In our dataset the attributes: anaemia, diabetes, high_blood_pressure, sex, smoking and DEATH_EVENT are qualitative variables. \n",
    "The rest are quantitative variables.\n",
    "Age attribute/variable is distributed between 40 to 95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d015600",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc43339",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", df.shape[0], \"observations and\", df.shape[1], \"columns in the dataset respectively.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d083bd",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA) and Feature Engineering\n",
    "\n",
    "In EDA, we will analyze the dataset to summarize the main characteristics and variables present in it. We have used statistical graphics and some data visualization methods to do so. \n",
    "The following topics have been covered under EDA:\n",
    "- Check for missing values.\n",
    "- Check for duplicate values.\n",
    "- Check the uniques values and their ranges for each object feature.\n",
    "- Print concise summary of dataset.\n",
    "- Plot graphs for each attribute.\n",
    "- Feature engineering: Check for good features through Correlation matrix.\n",
    "- Do some target feature analysis.\n",
    "- Identify our target variable : DEATH_EVENT\n",
    "- Detailed analysis of target variable/attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553aa8d",
   "metadata": {},
   "source": [
    "#### Firstly, we check for any missing values in the dataset.\n",
    "There are no missing values found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc1ef18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c136036f",
   "metadata": {},
   "source": [
    "#### Next, we check for any duplicate values and their sums in the dataset.\n",
    "There are zero duplicate values found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c86d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2031789f",
   "metadata": {},
   "source": [
    "#### Next, we check how many unique values object features contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e784ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "  print(f\"{col} has {df[col].nunique()} unique values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7894c4f7",
   "metadata": {},
   "source": [
    "#### Next, we print a concise summary of the dataset. \n",
    " The info() function prints information about the index dtype and column dtypes, non-null values and memory usage.\n",
    " Here in this case the size of the data set is (299,13) which means it has 299 entries and 13 columns respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40831a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb4d4ab",
   "metadata": {},
   "source": [
    "#### Plotting graphs for each of the attributes of the dataset,\n",
    " - age\n",
    " - anaemia\n",
    " - creatinine_phosphokinase\n",
    " - diabetes\n",
    " - ejection_fraction\n",
    " - high blood pressure\n",
    " - platelets\n",
    " - serum creatinine\n",
    " - serum_sodium\n",
    " - sex\n",
    " - smoking\n",
    " - time\n",
    " - death event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b7bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize = (20,20))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f05b3d",
   "metadata": {},
   "source": [
    "###  Features Engineering : Correlation Matrix\n",
    "A correlation matrix is a table which depicts the correlation coefficients between attributes of dataset. Here, we will plot a matrix to depict the various correlation factors amongst our dataset attributes.\n",
    "\n",
    "From the heatmap below, we can see that the three features most correlated (both positvely and negatively) with a patient's survival outcome (DEATH_EVENT) namely: ejection fraction, serum_creatinine and time. Therefore, while splitting the data into a train and test set, we can explicitly select the mentioned three features instead of the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0424243",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "data=df\n",
    "p=sns.heatmap(df.corr(), annot=True,cmap='RdYlGn',square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4225f01b",
   "metadata": {},
   "source": [
    "#### Taking two attributes(Diabetes and Age) and having a closer look at their plots. Below is a scatter plot which depicts the correlation between their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f02b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=df['diabetes'], y=df['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0cf2c",
   "metadata": {},
   "source": [
    "#### Next, we will plot a regression graph between the two attributes(Diabetes and Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=df['diabetes'], y=df['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a650273c",
   "metadata": {},
   "source": [
    "#### Death cases with respect to Age attribute are plotted below.\n",
    "\n",
    "The median for the 'Age' attribute is found to be around 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dd8c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.countplot(x =\"age\", data=df, hue =\"DEATH_EVENT\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad85ca2",
   "metadata": {},
   "source": [
    "#### Death Cases with respect to Sex attribute are plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"classic\")\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.countplot(x =\"sex\", data=df, hue =\"DEATH_EVENT\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046133e",
   "metadata": {},
   "source": [
    "#### Next, we check the true and false counts for DEATH_EVENT.\n",
    "\n",
    "We found that we have approximately 67% True values count and 32% False values count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a361b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts=df['DEATH_EVENT'].value_counts(normalize=True)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ab7f7",
   "metadata": {},
   "source": [
    "#### Next, we check the true and false counts for SEX.\n",
    "\n",
    "We found that we have approximately 64% True values count and 35% False values count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b5e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=df[\"sex\"].value_counts(normalize=True)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857c0a1",
   "metadata": {},
   "source": [
    "#### Next, we will check the percentage of the females and males that have survived and are dead and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f101b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_survive=df.age[(df.sex==0) & (df.DEATH_EVENT==0)].count()\n",
    "male_survive=df.age[(df.sex==1) & (df.DEATH_EVENT==0)].count()\n",
    "female_dead=df.age[(df.sex==0) & (df.DEATH_EVENT==1)].count()\n",
    "male_dead=df.age[(df.sex==1) & (df.DEATH_EVENT==1)].count()\n",
    "\n",
    "\n",
    "df=[female_survive,female_dead,male_survive,male_dead]\n",
    "label=[\"Female survive\",\"Female dead\",\"Male survive\",\"Male dead\"]\n",
    "explde=[0.1,0.1,0.1,0.1]\n",
    "colors = ['#eff7ff','#003d7a','#7a003d','#002b00']\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.pie(x=df,labels=label,autopct=\"%1.2f%%\",shadow=True,explode=explde,colors=colors)\n",
    "plt.suptitle(\"'DEATH_EVENT'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29cbdf1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Examining the target variable : Death_Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770faf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DEATH_EVENT'].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01746f8f",
   "metadata": {},
   "source": [
    "#### Next, we print the total count of the Death_Event attribute and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a231b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"DEATH_EVENT\"].value_counts())\n",
    "data[\"DEATH_EVENT\"].value_counts().plot(kind=\"pie\", autopct='%1.1f%%', figsize=(8,9));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ef033b",
   "metadata": {},
   "source": [
    "#### Now, we will check the categorical and numerical columns for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba341bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = data.loc(axis=1)['anaemia','diabetes','high_blood_pressure','sex','smoking','DEATH_EVENT']\n",
    "numerical_cols = data.loc(axis=1)['age','creatinine_phosphokinase','ejection_fraction','platelets','serum_creatinine','serum_sodium','time']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc03020",
   "metadata": {},
   "source": [
    "#### Categorical Columns :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75446725",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=0.90)\n",
    "\n",
    "fig, ax = plt.subplots(2,3, figsize=(15,13))\n",
    "[sns.countplot(data=data,x=cat, hue=\"DEATH_EVENT\", ax=ax.flatten()[i]) \n",
    " for i,cat in enumerate(categorical_cols.drop('DEATH_EVENT',axis=1))]\n",
    "ax.flatten()[-1].set_visible(False) # Remove the 6th empty plot\n",
    "[y_ax.set_ylim(0,140) for y_ax in ax.flatten()]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c6177",
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn_cols = ['#6a3fa0 ', '#d88138']\n",
    "fig = px.parallel_categories(categorical_cols, color=\"DEATH_EVENT\", color_continuous_scale=seaborn_cols)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad20a73",
   "metadata": {},
   "source": [
    "#### Analysis of Categorical Data:\n",
    "- Those having a high blood pressure have an increased risk of death. This is so because ususally hypertension is a major risk factor for heart failure.\n",
    "- Those having Anaemia is associated with increased risk of death. Again, this is so because anaemia is a common comorbidity of heart failure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca380188",
   "metadata": {},
   "source": [
    "#### Numerical Columns :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7224b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=4, ncols=2, figsize=(15,17))\n",
    "[sns.histplot(data=data, x=num, hue=\"DEATH_EVENT\", kde=True, ax=ax.flatten()[i]) for i,num in enumerate(numerical_cols)]\n",
    "ax.flatten()[-1].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775ca97",
   "metadata": {},
   "source": [
    "#### Analysis of Numerical Data:\n",
    "\n",
    "\n",
    "- A low value for time is highly correlated with death. This is due to the follow up period being cut short due to death of the patient. It is obvious however, that the number of days is not what killed the patient, it is merely that the number of days are cut short in those who were lost to follow up due to death. In this sense time is not a clinically useful variable for screening patients.\n",
    "- Elevated serum_creatinine appears to indicate inreased death.\n",
    "- A lower ejection fraction seems to have some correlation with increased death. This seems reasonable as typically a low stroke volume is associated with congestinve heart failure. Stroke volume, along with end diastolic volume make up the equation for ejection fraction. \n",
    "- Increased age appears to play a slight role in increased death.\n",
    "- Creatinine_phosphokinase, platelets, and serum_sodium do not appear to have any significant influence in predicting patient outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9060cf5",
   "metadata": {},
   "source": [
    "#### Next we will do some analysis on the target variable (Death_Event).\n",
    "\n",
    "We will print the percentages of the death event and not situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01f561",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['DEATH_EVENT']\n",
    "print(f'Percentage of DEATH EVENT: % {round(y.value_counts(normalize=True)[1]*100,2)} --> \\\n",
    "({y.value_counts()[1]} cases for DEATH EVENT)\\nPercentage of NOT DEATH EVENT: % {round(y.value_counts(normalize=True)[0]*100,2)} --> ({y.value_counts()[0]} cases for NOT Heart Disease)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0762c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['DEATH_EVENT']==0].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1373eb9",
   "metadata": {},
   "source": [
    "# 5. Model Selection and Training\n",
    "#### Now we move on to next step, the selection of different models and their respective training processes.\n",
    "#### We will start with feature scaling. In this process we basically normalize the range of independent features of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93b1c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "df_scaled = data.copy()\n",
    "df_scaled.loc(axis=1)[numerical_cols.columns] = scaler.fit_transform(data.loc(axis=1)[numerical_cols.columns]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6907780b",
   "metadata": {},
   "source": [
    "#### Next, we will do some Model comparison using Repeated Stratified K-Fold Cross Validation.\n",
    "\n",
    "Repeated k-fold cross-validation is a method used to improve the estimated performance of a machine learning model. This involves repeating the cross-validation procedure multiple times and reporting the mean result across all folds from all runs.\n",
    "\n",
    "We have considered the following models:\n",
    "- Logistic Regression\n",
    "- Random Forest Classifier\n",
    "- XGBoost Classifier\n",
    "- Gradient Boost Classifier\n",
    "- SVC\n",
    "- Quadratic Discriminant Analysis\n",
    "\n",
    "\n",
    "According to the range of independent features, training accuracy  and test scores we will decide which models to use for our further training and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444fa15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2 #Random state ensures that the splits we are tryin to generate are reproducible.\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=RANDOM_STATE)\n",
    "\n",
    "feat_set = ['Leakage', 'Baseline']\n",
    "\n",
    "# creating a list of models to evaluate.\n",
    "models = [LogisticRegression(), \n",
    "          RandomForestClassifier(random_state=RANDOM_STATE), \n",
    "          XGBClassifier(verbosity=0, use_label_encoder = False, \n",
    "                        random_state=RANDOM_STATE, eval_metric='logloss'),\n",
    "          GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "          SVC(kernel='sigmoid'),\n",
    "          QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "model_names = [mod.__class__.__name__ for mod in models]\n",
    "\n",
    "mod_cols = ['Name', \n",
    "            'Parameters',\n",
    "            'Time']\n",
    "\n",
    "df_mod = pd.DataFrame(columns=mod_cols)\n",
    "\n",
    "for i in range(len(feat_set)):\n",
    "\n",
    "    # Target variable feature considered.\n",
    "    \n",
    "    if (i==0):\n",
    "        X = df_scaled.drop('DEATH_EVENT',axis=1)\n",
    "    else:\n",
    "        X = df_scaled.drop(['time','DEATH_EVENT'],axis=1)\n",
    "\n",
    "     \n",
    "    y = df_scaled['DEATH_EVENT']\n",
    "    \n",
    "    for j,model in enumerate(models):\n",
    "\n",
    "        # Now, evaluating the models below.\n",
    "        cv_results = cross_validate(model, X, y, cv=cv, scoring=\"f1\", return_train_score = True)\n",
    "        df_mod.loc[j + len(models)*i , 'Parameters'] = str(model.get_params())\n",
    "        df_mod.loc[j + len(models)*i, 'Name'] = model.__class__.__name__\n",
    "        df_mod.loc[j + len(models)*i, 'Time'] = cv_results['fit_time'].mean()\n",
    "        df_mod.loc[j + len(models)*i, 'Train Accuracy'] = cv_results['train_score'].mean()\n",
    "        df_mod.loc[j + len(models)*i, 'Test Score'] = cv_results['test_score'].mean()\n",
    "        df_mod.loc[j + len(models)*i, 'feat_set'] = feat_set[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod.loc(axis=1)['Name','Train Accuracy','Test Score','feat_set'].sort_values('Test Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8a0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(data_frame = df_mod.sort_values('Test Score', ascending=True),\n",
    "             x=\"Name\", y=\"Test Score\", color=\"feat_set\", barmode=\"group\",\n",
    "             color_discrete_sequence=px.colors.qualitative.D3,\n",
    "             template = \"plotly_white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b873a58",
   "metadata": {},
   "source": [
    "Now, from above barplot we can see the importance of correct feature selection. The 'time' variable cannot be a feature of the model because it is not available at the time of prediction.\n",
    "The 'time' variable is a direct result of the death event so to use the time variable in the model would introduce target leakage.\n",
    "\n",
    "Using an example, let us assume a scenario that a patient goes to a doctor to receive a heart failure prediction. \n",
    "The doctor agrees and collects his data to make a prediction using our model. However, when the doctor comes across the time column, he will ponder so as to what to fill in there. If he considers the number of days then the question arises since what time period to consider(the start).\n",
    "Therefore, the model that we build must be useful and accurate and hence we cannot use time as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c2862e",
   "metadata": {},
   "source": [
    "#### From the above plot and results, \n",
    "#### XGBoost, Random Forest, and Gradient Boost are the most accurate classifiers for this data using default hyperparameters.\n",
    "#### Hence,I will consider these three models for further training and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf02d00f",
   "metadata": {},
   "source": [
    "Our dataset is unbalanced, so we will use Synthetic Minority Oversampling Technique to improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced83fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(y) \n",
    "\n",
    "# transform the dataset\n",
    "oversample = SMOTE()\n",
    "X_sm, y_sm = oversample.fit_resample(X, y)\n",
    "df_sm = X_sm.copy()\n",
    "df_sm['DEATH_EVENT'] = y_sm\n",
    "smote_counter = Counter(y_sm)\n",
    "\n",
    "## Visualise the oversampling \n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax = [ax1, ax2]\n",
    "for label, _ in counter.items():\n",
    "    row_ix = np.where(y == label)[0]\n",
    "    ax1.scatter(X.loc(axis=0)[row_ix][\"platelets\"],\n",
    "                    [row_ix], \n",
    "                    X.loc(axis=0)[row_ix][\"age\"],\n",
    "                    label=str(label),\n",
    "                    alpha=0.75)\n",
    "                    \n",
    "for label_sm, _ in smote_counter.items():\n",
    "    row_ix_sm = np.where(y_sm == label_sm)[0]\n",
    "    ax2.scatter(X_sm.loc(axis=0)[row_ix_sm][\"platelets\"], \n",
    "                    [row_ix_sm],\n",
    "                    X_sm.loc(axis=0)[row_ix_sm][\"age\"],\n",
    "                    label=str(label_sm), alpha=0.75)\n",
    "\n",
    "for axi in ax:\n",
    "    axi.set_zlim(-2,2)\n",
    "    axi.set_xlim(-2,2)\n",
    "    axi.set_ylim(0,400)\n",
    "    axi.set_xticks([-2,-1,0,1,2])\n",
    "    axi.set_yticks([0,100,200,300,400])\n",
    "    axi.set_zticks([-2,-1,0,1,2])\n",
    "    \n",
    "ax1.set_title(\"Original Data\")\n",
    "ax2.set_title(\"SMOTE Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819fce1",
   "metadata": {},
   "source": [
    "#### Now, we will evaluate the effect of Synthetic Minority Oversampling Technique on all our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6814f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod_sm = df_mod.copy()\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    cv_results = cross_validate(model, X_sm, y_sm, cv=cv, scoring=\"f1\", return_train_score = True)\n",
    "    \n",
    "    # Here,adding 1 to the max index instead of appending so I can pass everything as a dict()\n",
    "    df_mod_sm.loc(axis=0)[df_mod_sm.index.values.max()+1] = {\n",
    "            'Name':model.__class__.__name__,\n",
    "            'Parameters':str(model.get_params()),\n",
    "            'Time':cv_results['fit_time'].mean(),\n",
    "            'Train Accuracy':cv_results['train_score'].mean(),\n",
    "            'Test Score':cv_results['test_score'].mean(),\n",
    "            'feat_set':'SMOTE'\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aadc5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(data_frame = df_mod_sm.sort_values('Test Score', ascending=True),\n",
    "             x=\"Name\", y=\"Test Score\", color=\"feat_set\", barmode=\"group\",\n",
    "             color_discrete_sequence=px.colors.qualitative.D3,\n",
    "             template = \"plotly_white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94638a47",
   "metadata": {},
   "source": [
    "#### Here now, selecting appropriate features can result in improvements in model metrics through eliminating noisy features. We will now do some feature importance using permutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7466c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model_names = df_mod_sm.sort_values('feat_set', ascending=False).sort_values('Test Score', ascending=False)['Name'][:3].values\n",
    "top_models = [m for m in models if m.__class__.__name__ in top_model_names]\n",
    "\n",
    "# Initialize a DataFrame to contain the importances of each feature for each model\n",
    "df_imp = pd.DataFrame(index=range(0,len(X_sm.columns)*len(top_models)), columns=['feature','model','importance'])\n",
    "\n",
    "# len_feat will allow us to populate the features for each model\n",
    "len_feat = int(len(X_sm.columns))\n",
    "\n",
    "for i in range(len(top_models)):\n",
    "    results = permutation_importance(top_models[i].fit(X_sm, y_sm), X_sm, y_sm, scoring=\"f1\", \n",
    "                                n_repeats=10, n_jobs=None, \n",
    "                                random_state=RANDOM_STATE)   \n",
    "    df_imp.loc[range(len_feat*i,len_feat*(i+1)),'importance'] = (results['importances_mean'])\n",
    "    df_imp.loc[range(len_feat*i,len_feat*(i+1)),'model'] = top_models[i].__class__.__name__\n",
    "    df_imp.loc[range(len_feat*i,len_feat*(i+1)),'feature'] = X_sm.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f49be0",
   "metadata": {},
   "source": [
    "#### After doing the feature importance using different permutations we will plot a graph depicting the results for all our three models:\n",
    "\n",
    "- XGBClassifier\n",
    "- Gradient Boost Claissifier\n",
    "- Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc0de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(data_frame=df_imp.sort_values('importance'), \n",
    "             x=\"importance\", \n",
    "             y=\"feature\", \n",
    "             color=\"model\",\n",
    "             barmode=\"group\",\n",
    "             orientation=\"h\",\n",
    "             template = \"plotly_white\"\n",
    "             )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a15f35d",
   "metadata": {},
   "source": [
    "#### Next, I am doing feature importance using SHAP (SHapley Additive exPlanations).\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of a machine learning model. \n",
    "It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions.\n",
    "Below we can see the different high and low feature values for all our attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03bc453",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(top_models[2].fit(X_sm, y_sm))\n",
    "shap_values = explainer.shap_values(X_sm)\n",
    "shap.summary_plot(shap_values, features=X_sm, feature_names=X_sm.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df7bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, features=X_sm, \n",
    "                        feature_names=X_sm.columns, \n",
    "                        plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aae155",
   "metadata": {},
   "source": [
    "#### From the plots above, it is clear that  'high_bloodpressure', 'smoking', 'diabetes' and, 'anaemia' are the least important features across both methods of determining feature importance (permutation & SHAP).\n",
    "#### Hence, these will be dropped from the analysis and the following change in performance will be noted.\n",
    "\n",
    "And next, we will run the models using two feature sets namely:\n",
    "- mean absolute SHAP value >0.1\n",
    "- mean absolute SHAP value >0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf4a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "\n",
    "df_mod_sm_f = df_mod_sm[df_mod_sm['Name'].isin(top_model_names)].copy()\n",
    "\n",
    "for j in [1,6]:\n",
    "    feats.append(X_sm.columns[np.argsort(np.abs(shap_values).mean(0))][::-1][:j+1].values)\n",
    "\n",
    "for i,feat in enumerate(feats):    \n",
    "    for model in top_models:\n",
    "        cv_results = cross_validate(model, X_sm.loc(axis=1)[feat], y_sm, cv=cv, \n",
    "                                    scoring=\"f1\", return_train_score = True)\n",
    "\n",
    "        # Adding 1 to the max index instead of appending so I can pass everything as a dict()\n",
    "        df_mod_sm_f.loc(axis=0)[df_mod_sm_f.index.values.max()+1] = {\n",
    "                'Name':model.__class__.__name__,\n",
    "                'Parameters':str(model.get_params()),\n",
    "                'Time':cv_results['fit_time'].mean(),\n",
    "                'Train Accuracy':cv_results['train_score'].mean(),\n",
    "                'Test Score':cv_results['test_score'].mean(),\n",
    "                'feat_set': f\"SMOTE {len(feat)}-Feature\"\n",
    "                 }\n",
    "fig = px.bar(data_frame = df_mod_sm_f.sort_values('Test Score', ascending=True),\n",
    "             x=\"Name\", y=\"Test Score\", color=\"feat_set\", barmode=\"group\",\n",
    "             color_discrete_sequence=px.colors.qualitative.D3,\n",
    "             template = \"plotly_white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e530f38",
   "metadata": {},
   "source": [
    "#### From the results obtained above we conclude,\n",
    "- A identical F1 score is achieved using only 7 of the original features as compared to the full set of features.\n",
    "- Making use of only 2 features results in a marginal decrease in the F1 score as compared to using 7 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c155a9",
   "metadata": {},
   "source": [
    "# 6. Hyper Parameter Tuning\n",
    "In this section we will do some hyper parameter tuning on all the three models using Grid Search.\n",
    "\n",
    "Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters. It is an exhaustive search that is performed on a the specific parameter values of a model. (source: Wikipedia)\n",
    "\n",
    "#### NOTE:\n",
    "     While running this section it may take upto 2-7 minutes for each classifier. Please be patient :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb8a97a",
   "metadata": {},
   "source": [
    "### Random Forest Classifier Hyperparameter Tuning using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71b1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_space = {\"n_estimators\" : [2000, 5000],\n",
    "                  \"criterion\":[\"gini\"], \n",
    "                  \"max_depth\":[9, 18],\n",
    "                  \"min_samples_split\": [2, 6]}\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "rf_gscv = HalvingGridSearchCV(estimator=random_forest, param_grid=rf_param_space,\n",
    "                   cv = 10, scoring = \"f1\", n_jobs = -1,verbose = 1, random_state=RANDOM_STATE)\n",
    "\n",
    "# rf_gscv = GridSearchCV(estimator=random_forest, param_grid=rf_param_space,\n",
    "#                    cv = cv, n_jobs=-1, scoring = \"f1\", verbose = 1)\n",
    "\n",
    "print('Running GridSearchCV for Random Forest Classifier...')\n",
    "rf_gscv.fit(X_sm.loc(axis=1)[feats[1]], y_sm)\n",
    "\n",
    "print(\"best estimator: \", rf_gscv.best_estimator_)\n",
    "print(\"best parameters: \", rf_gscv.best_params_)\n",
    "print(\"best score: \",rf_gscv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7c762b",
   "metadata": {},
   "source": [
    "### Gradient Boosing Classifier Hyperparameter Tuning using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dda3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_param_space = {\"n_estimators\" : [1000, 2500],\n",
    "                  \"subsample\" : [0.5, 1.0],\n",
    "                  \"max_depth\":[9, 18],\n",
    "                  \"learning_rate\" : [0.025, 0.75]}\n",
    "\n",
    "grad_boost = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "gb_gscv = HalvingGridSearchCV(estimator=grad_boost, param_grid=gb_param_space,\n",
    "                   cv = 10, scoring = \"f1\", n_jobs = -1,verbose = 1, random_state=RANDOM_STATE)\n",
    "\n",
    "# gb_gscv = GridSearchCV(estimator=grad_boost, param_grid=gb_param_space,\n",
    "#                    cv = cv, n_jobs=-1, scoring = \"f1\", verbose = 1)\n",
    "\n",
    "print('Running GridSearchCV for Gradient Boosting Classifier...')\n",
    "gb_gscv.fit(X_sm.loc(axis=1)[feats[1]], y_sm)\n",
    "\n",
    "print(\"best estimator: \", gb_gscv.best_estimator_)\n",
    "print(\"best parameters: \", gb_gscv.best_params_)\n",
    "print(\"best score: \", gb_gscv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa086726",
   "metadata": {},
   "source": [
    "### XGBoost Hyperparameter using Grid Search\n",
    "\n",
    "For this classifier, the hyperparameter tuning was pretty slow and resulted in a decrease in F1 score. Hence, I have used default hyperparameters and also the .fit statement in the code below is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1624a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param_space = {\"n_estimators\":[100, 500],\n",
    "                   \"subsample\" : [0.5, 1.0],\n",
    "                   \"max_depth\" : [9, 18],\n",
    "                   \"eta\" : [0.25, 0.75]}\n",
    "\n",
    "xgboost = XGBClassifier(use_label_encoder=False, random_state=RANDOM_STATE, eval_metric='logloss')\n",
    "\n",
    "# xgb_gscv = HalvingGridSearchCV(estimator=xgboost, param_grid=xgb_param_space,\n",
    "#                    cv = cv, scoring = \"f1\", n_jobs = -1,verbose = 1, random_state=RANDOM_STATE)\n",
    "\n",
    "xgb_gscv = GridSearchCV(estimator=xgboost, param_grid=xgb_param_space,\n",
    "                   cv = 10, n_jobs=-1, scoring = \"f1\", verbose = 1)\n",
    "\n",
    "# ------- Uncomment code below to run grid search for XGBoost -------\n",
    "# print('Running GridSearchCV for XGBoost Classifier...')\n",
    "# xgb_gscv.fit(X_sm.loc(axis=1)[feats[1]], y_sm)\n",
    "\n",
    "# print(\"best estimator: \", xgb_gscv.best_estimator_)\n",
    "# print(\"best parameters: \", xgb_gscv.best_params_)\n",
    "# print(\"best score: \", xgb_gscv.best_score_)\n",
    "\n",
    "\n",
    "# xgb_gscv.best_params_['use_label_encoder'] = False\n",
    "# xgb_gscv.best_params_['eval_metric'] = 'logloss'\n",
    "# xgb_gscv.best_params_['random_state'] = RANDOM_STATE\n",
    "# tuned_xgb = XGBClassifier(**xgb_gscv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d17a3d",
   "metadata": {},
   "source": [
    "### Prediction using the tuned models on Test Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3aabe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gscv.best_params_['random_state'] = RANDOM_STATE\n",
    "gb_gscv.best_params_['random_state'] = RANDOM_STATE\n",
    "\n",
    "tuned_rf = RandomForestClassifier(**rf_gscv.best_params_)\n",
    "tuned_gb = GradientBoostingClassifier(**gb_gscv.best_params_)\n",
    "tuned_xgb = XGBClassifier(verbosity=0, use_label_encoder = False, \n",
    "                        random_state=RANDOM_STATE, eval_metric='logloss')\n",
    "\n",
    "tuned_models = [tuned_rf, tuned_gb, tuned_xgb] \n",
    "\n",
    "df_final = pd.DataFrame(index = range(len(tuned_models)) ,\n",
    "                        columns = ['Name',\n",
    "                                   'Parameters',\n",
    "                                   'Time', \n",
    "                                   'F1 Score', \n",
    "                                   'Precision', \n",
    "                                   'Recall',\n",
    "                                   'Accuracy',\n",
    "                                   'roc_auc',\n",
    "                                   'fpr',\n",
    "                                   'tpr'\n",
    "                                  ])\n",
    "\n",
    "for i, model in enumerate(tuned_models):\n",
    "    cv_results = cross_validate(model, X_sm.loc(axis=1)[feats[1]], y_sm, cv=cv, \n",
    "                                scoring=[\"f1\", \"precision\", \"recall\", \"roc_auc\", \"accuracy\"],\n",
    "                                return_train_score = True)\n",
    "\n",
    "    # Adding 1 to the max index instead of appending so I can pass everything as a dict()\n",
    "    df_mod_sm_f.loc(axis=0)[df_mod_sm_f.index.values.max()+1] = {\n",
    "            'Name':model.__class__.__name__,\n",
    "            'Parameters':str(model.get_params()),\n",
    "            'Time':cv_results['fit_time'].mean(),\n",
    "            'Train Accuracy':cv_results['train_accuracy'].mean(),\n",
    "            'Test Score':cv_results['test_f1'].mean(),\n",
    "            'feat_set': f\"SMOTE {len(feats[1])}-Feature (Tuned)\"\n",
    "             }\n",
    "    # Gather data needed for ROC Curves\n",
    "    y_pred = cross_val_predict(model, X_sm.loc(axis=1)[feats[1]], y_sm, cv=10, method='predict_proba')  \n",
    "    fpr, tpr, thresholds = roc_curve(y_sm, y_pred[:,1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Adding 1 to the max index instead of appending so everything can pass as a dict()\n",
    "    df_final.loc(axis=0)[i] = {\n",
    "            'Name':model.__class__.__name__,\n",
    "            'Parameters':str(model.get_params()),\n",
    "            'Time':cv_results['fit_time'].mean(),            \n",
    "            'F1 Score':cv_results['test_f1'].mean(),\n",
    "            'Precision':cv_results['test_precision'].mean(),\n",
    "            'Recall':cv_results['test_recall'].mean(),\n",
    "            'Accuracy':cv_results['test_accuracy'].mean(),\n",
    "            'roc_auc':roc_auc,\n",
    "            'fpr':fpr,\n",
    "            'tpr':tpr\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d8e753",
   "metadata": {},
   "source": [
    "#### Plotting the results of the Test dataset.\n",
    "In the plot below we are able to see the test scores for all the three models for all feature sets namely,\n",
    "- Baseline\n",
    "- Leakage\n",
    "- SMOTE 2-feature\n",
    "- SMOTE\n",
    "- SMOTE 7-feature\n",
    "- SMOTE 7-feature Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58e2f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(data_frame = df_mod_sm_f.sort_values('Test Score', ascending=True),\n",
    "             x=\"Name\", y=\"Test Score\", color=\"feat_set\", barmode=\"group\",\n",
    "             color_discrete_sequence=px.colors.qualitative.D3,\n",
    "             template = \"plotly_white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99413070",
   "metadata": {},
   "source": [
    "#### Next, we will plot a graph depicting the false positive rates and true positive rates for all the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd93f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lw=3.5\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "[plt.plot(df_final['fpr'][i], df_final['tpr'][i], \n",
    "          label = f\"{df_final['Name'][i]} - AUC:{df_final['roc_auc'][i]:.2f}\",\n",
    "          linewidth=lw, alpha=0.75) for i in range(len(df_final))]\n",
    "\n",
    "plt.plot([0,1],[0,1],'--k', alpha=0.5)\n",
    "plt.xlim(0,1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.ylim(0,1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5615d93",
   "metadata": {},
   "source": [
    "# 7. Final Model \n",
    "Now, we will have to analyse and check for the final model which performs the best and has the highest F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef99e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_melt = df_final.melt(value_vars = [\n",
    "    \"F1 Score\", \n",
    "    \"Precision\", \n",
    "    \"Recall\", \n",
    "    \"roc_auc\", \n",
    "    \"Accuracy\"], id_vars=\"Name\")\n",
    "\n",
    "fig = px.bar(data_frame = df_final_melt.sort_values('value', ascending=True),\n",
    "             x=\"variable\", y=\"value\", color=\"Name\", barmode=\"group\",\n",
    "             color_discrete_sequence=px.colors.qualitative.Plotly,\n",
    "             template = \"plotly_white\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c057c39b",
   "metadata": {},
   "source": [
    "# 8. Results and Conclusion\n",
    "\n",
    "#### - From the above results we can confidently say that Random Forest Classifier performs the best amongst all the models.\n",
    "#### - The real world application of this model will mean that it should be optimised to minimize the number of false negatives predictions.\n",
    "#### - The best metric to be considered is Recall (TP / TP + FN). And in our case, the Recall score is highest for the RandomForestClassifier. Also, every other metric of RandomForestClassifier is the highest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
